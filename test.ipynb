{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "19Uc4b5OQ8FrwLg6y5CjGdN9lq9niboEr",
      "authorship_tag": "ABX9TyOFyH6a3odARYMoz+xxItjt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahranWidebot/open_universal_arabic_asr_leaderboard/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install speechbrain\n",
        "!pip install Cython\n",
        "!pip install packaging\n",
        "!pip install huggingsound\n",
        "#!pip install \"nemo_toolkit[all]\"\n",
        "!pip install pyctcdecode\n",
        "!pip install qwen-omni-utils\n",
        "!pip install omnilingual-asr\n",
        "!pip install clearvoice\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PWCLMeTyjwrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "!pip install \"nemo_toolkit[all]==1.17.0\"\n",
        "\n",
        "!pip install pyctcdecode tqdm huggingsound transformers speechbrain\n",
        "\n",
        "# Install PyTorch (Colab usually has a compatible version, but let's ensure it)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install NVIDIA NeMo for ASR\n",
        "!pip install nemo-toolkit[all]\n",
        "!pip install soundfile\n",
        "!pip install torchcodec\n",
        "!pip install \"ffmpeg\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4s-72HDSozQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO0v5CEUk5kt",
        "outputId": "c59b5239-cbdf-417e-e4e5-456e6b6e9dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchcodec\n",
            "  Downloading torchcodec-0.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Downloading torchcodec-0.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchcodec\n",
            "Successfully installed torchcodec-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyTorch (Colab usually has a compatible version, but let's ensure it)\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install NVIDIA NeMo for ASR\n",
        "!pip install nemo-toolkit[all]\n",
        "!pip install soundfile"
      ],
      "metadata": {
        "id": "TjhTAgkojukU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.10 python3.10-dev python3.10-distutils -y\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n"
      ],
      "metadata": {
        "id": "2O-oe_53V2Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Natural-Language-Processing-Elm/open_universal_arabic_asr_leaderboard\n"
      ],
      "metadata": {
        "id": "vvGCicabaxUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4hEZHCMPnbEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n"
      ],
      "metadata": {
        "id": "w8ZLvejCqXrE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "# Load dataset without decoding audio\n",
        "ds = load_dataset(\"UBC-NLP/Casablanca\", \"Egypt\", streaming=False)"
      ],
      "metadata": {
        "id": "tC-090mkETDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTR0bPzOICHl",
        "outputId": "63813a16-4285-46d1-e2f1-99a5ea44c67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['audio', 'seg_id', 'transcription', 'gender', 'duration'],\n",
              "        num_rows: 846\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['audio', 'seg_id', 'transcription', 'gender', 'duration'],\n",
              "        num_rows: 846\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "import os, json, soundfile as sf\n",
        "\n",
        "data_folder = \"casablanca_test_audio\"\n",
        "os.makedirs(data_folder, exist_ok=True)\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "ds = load_dataset(\"UBC-NLP/Casablanca\", \"Egypt\", streaming=True)\n",
        "\n",
        "def save_audio_files(split_name):\n",
        "    manifest_path = f\"{split_name}_manifest.json\"\n",
        "    split_folder = os.path.join(data_folder, split_name)\n",
        "    os.makedirs(split_folder, exist_ok=True)\n",
        "\n",
        "    split = ds[split_name]\n",
        "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for example in split:\n",
        "            # Load audio on-the-fly\n",
        "            audio_path = example[\"audio\"][\"path\"]  # path to local/streamed file\n",
        "            audio_array, sample_rate = sf.read(audio_path)\n",
        "\n",
        "            filename = f\"{example['seg_id']}.wav\"\n",
        "            save_path = os.path.join(split_folder, filename)\n",
        "            sf.write(save_path, audio_array, sample_rate)\n",
        "\n",
        "            item = {\n",
        "                \"audio_filepath\": \"{data_folder}/\" + filename,\n",
        "                \"duration\": len(audio_array)/sample_rate,\n",
        "                \"text\": example.get(\"transcription\", \"\")\n",
        "            }\n",
        "            json.dump(item, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")\n",
        "    print(f\"✔ Saved {split_name} manifest: {manifest_path}\")\n",
        "\n",
        "# Save splits\n",
        "save_audio_files(\"validation\")\n",
        "save_audio_files(\"test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHURmuBOHrh4",
        "outputId": "8753fa0e-c56e-4989-e9a8-88b687c3bb4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "%cd /content/open_universal_arabic_asr_leaderboard\n",
        "sys.path.append(\"/content/open_universal_arabic_asr_leaderboard\")\n",
        "\n",
        "from models.whisper import run_whisper\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHyBNJbG_j6y",
        "outputId": "753a2c24-a5d0-46a2-fa22-831a14d8a7a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/open_universal_arabic_asr_leaderboard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of run_omnilingual\n",
        "\n",
        "# Path to the OmniLingual model you want to use\n",
        "# Options: \"omniASR_CTC_300M\", \"omniASR_LLM_300M\", \"omniASR_CTC_1B\", \"omniASR_LLM_1B\",\n",
        "#          \"omniASR_CTC_3B\", \"omniASR_LLM_3B\", \"omniASR_CTC_7B\", \"omniASR_LLM_7B\"\n",
        "model_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "# Path to the manifest you created earlier\n",
        "data_manifest = \"/content/test_manifest.json\"\n",
        "\n",
        "# The folder where your audio files are stored\n",
        "data_folder = \"/content/casablanca_test_audio/test\"\n",
        "\n",
        "# Path to save the output manifest (predictions)\n",
        "output_manifest = \"casablanca_test_predictions.json\"\n",
        "\n",
        "# Run the OmniLingual ASR inference\n",
        "run_whisper(model_id, data_manifest, data_folder, output_manifest)\n",
        "\n",
        "print(f\"Inference complete! Predictions saved to {output_manifest}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "UjxaJi2vENCm",
        "outputId": "b8489ea8-c132-4e37-a2ba-97492c028de0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n",
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2538359810.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Run the OmniLingual ASR inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mrun_whisper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_manifest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_manifest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inference complete! Predictions saved to {output_manifest}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open_universal_arabic_asr_leaderboard/models/whisper.py\u001b[0m in \u001b[0;36mrun_whisper\u001b[0;34m(model_id, data_manifest, data_folder, output_manifest)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;31m#print(\"average rtf : \", all_inference_time/all_audio_duration)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;31m#print(\"model memory : \", initial_memory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m#print(\"average inference-only memory : \", sum(all_inference_memory)/len(all_inference_memory))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}